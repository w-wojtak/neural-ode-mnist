{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3033285-b515-49f8-9230-57b937b2dd08",
   "metadata": {},
   "source": [
    "Neural ODEs for MNIST Classification\n",
    "===================================\n",
    "\n",
    "This code implements Neural Ordinary Differential Equations (Neural ODEs) for MNIST digit classification.\n",
    "Key concepts:\n",
    "- Neural ODEs are continuous-depth models that generalize ResNets\n",
    "- Instead of discrete layers, they use continuous transformations\n",
    "- The transformation is defined by an ODE: dx/dt = f(t,x)\n",
    "- The ODE is solved using numerical methods\n",
    "\n",
    "Reference: \"Neural Ordinary Differential Equations\" (Chen et al., 2018)\n",
    "https://arxiv.org/abs/1806.07366"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a7ff8d-1aef-4017-b9ce-2f9d35d1fd89",
   "metadata": {},
   "source": [
    "Setup and Imports\n",
    "===================================\n",
    "\n",
    "We use PyTorch for deep learning, torchvision for the MNIST dataset, and torchdiffeq for the ODE solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6137e13-34ba-4474-8507-55596fee27a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchdiffeq import odeint  # ODE solver for Neural ODEs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bcbd6c-ce71-4cbf-b497-db1e8f412a2b",
   "metadata": {},
   "source": [
    "The ODE Function\n",
    "===================================\n",
    "\n",
    "The core of a Neural ODE is the function that describes how the hidden state evolves over time. This is analogous to \n",
    "𝑓\n",
    "(\n",
    "𝑡\n",
    ",\n",
    "𝑥\n",
    ")\n",
    "f(t,x) in the ODE\n",
    "\n",
    "𝑑\n",
    "𝑥\n",
    "𝑑\n",
    "𝑡\n",
    "=\n",
    "𝑓\n",
    "(\n",
    "𝑡\n",
    ",\n",
    "𝑥\n",
    ")\n",
    "dt\n",
    "dx\n",
    "​\n",
    " =f(t,x)\n",
    "We implement this as a small neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1961a3ce-d2a5-4664-a5bb-8434d483380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ODEFunc, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28 * 28)\n",
    "        )\n",
    "\n",
    "    def forward(self, t, x):\n",
    "        # t: time (not used here, but required by the ODE solver)\n",
    "        # x: current state\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918d3408-03b7-4d3c-8e30-577fec5c18c0",
   "metadata": {},
   "source": [
    "The ODE Block\n",
    "===================================\n",
    "\n",
    "The ODE Block wraps the ODE function and uses a numerical solver to compute the transformation from the initial state to the final state. This acts like a continuous-depth layer in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afd714c3-0be9-4950-9c0a-5ccae38e3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEBlock(nn.Module):\n",
    "    def __init__(self, odefunc):\n",
    "        super(ODEBlock, self).__init__()\n",
    "        self.odefunc = odefunc\n",
    "        self.integration_time = torch.tensor([0, 1])  # Integrate from t=0 to t=1\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.integration_time = self.integration_time.type_as(x)\n",
    "        out = odeint(self.odefunc, x, self.integration_time, method='dopri5')\n",
    "        return out[1]  # Return the final state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433b20d-260e-4535-a7a1-9c4a6b8b3693",
   "metadata": {},
   "source": [
    "The Complete Neural ODE Model\n",
    "===================================\n",
    "\n",
    "The full model flattens the MNIST image, applies the Neural ODE transformation, and then classifies the result with a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "857d228d-c921-43d2-b437-aba4878ac88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralODE, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.odeblock = ODEBlock(ODEFunc())\n",
    "        self.classifier = nn.Linear(28 * 28, 10)  # 10 classes for MNIST\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.odeblock(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923388fb-1952-476b-ade3-ce40722a97d2",
   "metadata": {},
   "source": [
    "Data Loading\n",
    "===================================\n",
    "\n",
    "We load the MNIST dataset, normalize it, and create data loaders for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afe7971c-d5fc-4fe6-936a-c5e6dab91575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_loaders(batch_size=128):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=True, transform=transform, download=True)\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=False, transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f6df72-40ab-46c4-bcaa-5fb4a53968a6",
   "metadata": {},
   "source": [
    "Training and Evaluation Functions\n",
    "===================================\n",
    "\n",
    "These functions handle the training and evaluation of the model. The training function updates the model parameters, while the evaluation function computes accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dd5e0a8-877c-4709-9141-48b6f08ca3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Batch: {batch_idx}/{len(train_loader)} Loss: {loss.item():.6f}')\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'Test set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2209bf67-b8f3-4585-a357-ebc176d57985",
   "metadata": {},
   "source": [
    "Main Training Loop\n",
    "===================================\n",
    "\n",
    "Finally, we put everything together: create the model, optimizer, and loss function, then train and evaluate for several epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be043c03-761f-4862-bf8a-8c356d33a27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Epoch: 1/10\n",
      "Batch: 0/469 Loss: 2.575295\n",
      "Batch: 100/469 Loss: 0.322705\n",
      "Batch: 200/469 Loss: 0.330603\n",
      "Batch: 300/469 Loss: 0.079806\n",
      "Batch: 400/469 Loss: 0.061047\n",
      "Test set: Average loss: 0.1102, Accuracy: 9637/10000 (96.37%)\n",
      "\n",
      "Epoch: 2/10\n",
      "Batch: 0/469 Loss: 0.050274\n",
      "Batch: 100/469 Loss: 0.087400\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m         evaluate(model, test_loader, criterion, device)\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     evaluate(model, test_loader, criterion, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, criterion, device)\u001b[39m\n\u001b[32m      4\u001b[39m data, target = data.to(device), target.to(device)\n\u001b[32m      5\u001b[39m optimizer.zero_grad()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m loss = criterion(output, target)\n\u001b[32m      8\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\VSCode Projects\\neural-ode-mnist\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\VSCode Projects\\neural-ode-mnist\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mNeuralODE.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m      9\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.flatten(x)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43modeblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classifier(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\VSCode Projects\\neural-ode-mnist\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\VSCode Projects\\neural-ode-mnist\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mODEBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mself\u001b[39m.integration_time = \u001b[38;5;28mself\u001b[39m.integration_time.type_as(x)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     out = \u001b[43modeint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43modefunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintegration_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdopri5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\VSCode Projects\\neural-ode-mnist\\venv\\Lib\\site-packages\\torchdiffeq\\_impl\\odeint.py:80\u001b[39m, in \u001b[36modeint\u001b[39m\u001b[34m(func, y0, t, rtol, atol, method, options, event_fn)\u001b[39m\n\u001b[32m     77\u001b[39m solver = SOLVERS[method](func=func, y0=y0, rtol=rtol, atol=atol, **options)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     solution = \u001b[43msolver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintegrate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     82\u001b[39m     event_t, solution = solver.integrate_until_event(t[\u001b[32m0\u001b[39m], event_fn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\VSCode Projects\\neural-ode-mnist\\venv\\Lib\\site-packages\\torchdiffeq\\_impl\\solvers.py:34\u001b[39m, in \u001b[36mAdaptiveStepsizeODESolver.integrate\u001b[39m\u001b[34m(self, t)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28mself\u001b[39m._before_integrate(t)\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(t)):\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     solution[i] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_advance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m solution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\VSCode Projects\\neural-ode-mnist\\venv\\Lib\\site-packages\\torchdiffeq\\_impl\\rk_common.py:246\u001b[39m, in \u001b[36mRKAdaptiveStepsizeODESolver._advance\u001b[39m\u001b[34m(self, next_t)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m next_t > \u001b[38;5;28mself\u001b[39m.rk_state.t1:\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m n_steps < \u001b[38;5;28mself\u001b[39m.max_num_steps, \u001b[33m'\u001b[39m\u001b[33mmax_num_steps exceeded (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m>=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m.format(n_steps, \u001b[38;5;28mself\u001b[39m.max_num_steps)\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[38;5;28mself\u001b[39m.rk_state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_adaptive_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrk_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m     n_steps += \u001b[32m1\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _interp_evaluate(\u001b[38;5;28mself\u001b[39m.rk_state.interp_coeff, \u001b[38;5;28mself\u001b[39m.rk_state.t0, \u001b[38;5;28mself\u001b[39m.rk_state.t1, next_t)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\VSCode Projects\\neural-ode-mnist\\venv\\Lib\\site-packages\\torchdiffeq\\_impl\\rk_common.py:340\u001b[39m, in \u001b[36mRKAdaptiveStepsizeODESolver._adaptive_step\u001b[39m\u001b[34m(self, rk_state)\u001b[39m\n\u001b[32m    338\u001b[39m t_next = t1\n\u001b[32m    339\u001b[39m y_next = y1\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m interp_coeff = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_interp_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_next\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m on_step_t:\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.next_step_index != \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.step_t) - \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\VSCode Projects\\neural-ode-mnist\\venv\\Lib\\site-packages\\torchdiffeq\\_impl\\rk_common.py:364\u001b[39m, in \u001b[36mRKAdaptiveStepsizeODESolver._interp_fit\u001b[39m\u001b[34m(self, y0, y1, k, dt)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Fit an interpolating polynomial to the results of a Runge-Kutta step.\"\"\"\u001b[39;00m\n\u001b[32m    363\u001b[39m dt = dt.type_as(y0)\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m y_mid = y0 + \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.view_as(y0)\n\u001b[32m    365\u001b[39m f0 = k[..., \u001b[32m0\u001b[39m]\n\u001b[32m    366\u001b[39m f1 = k[..., -\u001b[32m1\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    model = NeuralODE().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_loader, test_loader = get_mnist_loaders()\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch: {epoch+1}/{num_epochs}')\n",
    "        train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fba7d8-fd55-4a9e-9318-e8d5d2ae3280",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
